{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Sequences\n",
    "\n",
    "### Recognizing digit sequences from a simple synthetic dataset using TensorFlow \n",
    "\n",
    "---\n",
    "\n",
    "The following notebook details my implementation of a Convolutional Neural Network to recognize sequences of digits in a synthetic dataset created from the MNIST dataset. The purpose of the notebook is to understand how the architecture of a network has to be changed to accomodate classification of multiple objects.\n",
    "\n",
    "I have broken the notebook into two parts as TensorFlow programs are usually structured into a **construction phase**, that assembles a graph, and an **execution phase** that uses a session to execute ops in the graph.\n",
    "\n",
    "Let's start by importing some libraries and load our prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16.0, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Tensorflow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Let's load the greyscale images created in our previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/svhn_multi.h5','r')\n",
    "\n",
    "X_train = h5f['train_dataset'][:]\n",
    "y_train = h5f['train_labels'][:]\n",
    "X_test = h5f['valid_dataset'][:]\n",
    "y_test = h5f['valid_labels'][:]\n",
    "X_val = h5f['test_dataset'][:]\n",
    "y_val = h5f['test_labels'][:]\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_test.shape, y_test.shape)\n",
    "print('Test set', X_val.shape, y_val.shape)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "A helper function is a function that performs part of the computation of another function. Helper functions are used to make your programs easier to read by giving descriptive names to computations. They also let you reuse computations, just as with functions in general.\n",
    "\n",
    "### Helper function for plotting images\n",
    "\n",
    "Here is a simple helper function that will help us plot ``nrows`` * ``ncols``Â images and their true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, nrows, ncols, cls_true, cls_pred=None):\n",
    "    \n",
    "    # Initialize figure\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 2*nrows))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat): \n",
    "        \n",
    "        # Pretty string with actual number\n",
    "        true_number = ''.join(str(x) for x in cls_true[i] if x != 10)\n",
    "        \n",
    "        if cls_pred is None:\n",
    "            title = \"True: {0}\".format(true_number)\n",
    "        else:\n",
    "            # Pretty string with predicted number\n",
    "            pred_number = ''.join(str(x) for x in cls_pred[i] if x != 10)\n",
    "            title = \"True: {0}, Pred: {1}\".format(true_number, pred_number) \n",
    "            \n",
    "        ax.imshow(images[i,:,:,0], cmap='binary')\n",
    "        ax.set_title(title)   \n",
    "        ax.set_xticks([]); ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it to make sure everything is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the first 9 images from the training set\n",
    "images = X_train\n",
    "\n",
    "# Get the true classes for the images\n",
    "cls_true = y_train\n",
    "\n",
    "# Plot the images\n",
    "plot_images(images, 2, 6, cls_true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for creating new variables\n",
    "\n",
    "Functions for creating new [``TensorFlow Variables``](https://www.tensorflow.org/how_tos/variables/) in the given shape and initializing them with random values. Note that the initialization is not actually done at this point, it is merely being defined in the TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_weight_variable(name, shape):\n",
    "    return tf.get_variable(name, shape=shape,initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "def fc_weight_variable(name, shape):\n",
    "    return tf.get_variable(name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for creating a Convolutional Layer\n",
    "\n",
    "This function creates a new convolutional layer in the computational graph for TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input,             # The previous layer.\n",
    "                n_channels,       # Num. channels in prev. layer.\n",
    "                f_size,           # Width and height of each filter.\n",
    "                n_filters,        # Number of filters.\n",
    "                weight_name,      # Name of variable containing the weights\n",
    "                pooling=True):    # Use 2x2 max-pooling.\n",
    "\n",
    "    # Create weights and biases\n",
    "    weights = conv_weight_variable(weight_name, [f_size, f_size, n_channels, n_filters])\n",
    "    biases = bias_variable([n_filters])\n",
    "    \n",
    "    layer = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    layer = tf.nn.relu(layer + biases)\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if pooling:\n",
    "        layer = tf.nn.max_pool(layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='VALID')\n",
    "\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for flattening a layer\n",
    "\n",
    "A convolutional layer produces an output tensor with 4 dimensions. We will add fully-connected layers after the convolution layers, so we need to reduce the 4-dim tensor to 2-dim which can be used as input to the fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    \n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # Return the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for creating a new Fully-Connected Layer\n",
    "\n",
    "This function creates a new fully-connected layer in the computational graph for TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(input,          # The previous layer.\n",
    "            num_inputs,      # Num. inputs from prev. layer.\n",
    "            num_outputs,     # Num. outputs.\n",
    "            weight_name,     # Name of variable containing the weights\n",
    "            relu=True):      # Use Rectified Linear Unit (ReLU)?\n",
    "    \n",
    "    # Create new weights and biases.\n",
    "    weights = fc_weight_variable(weight_name, shape=[num_inputs, num_outputs])\n",
    "    biases = bias_variable([num_outputs])\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for creating a new Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_function(input,         # Previous layer\n",
    "                     num_inputs,    # Number of inputs from previous layer\n",
    "                     num_outputs,   # Number of outputs\n",
    "                     weight_name):  # Name of variable containing the weights\n",
    "                 \n",
    "    # Create weights and biases\n",
    "    weights = fc_weight_variable(weight_name, [num_inputs, num_outputs])\n",
    "    biases = bias_variable([num_outputs])\n",
    "    \n",
    "    # Softmax\n",
    "    logits = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    return logits, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dimensions are used in several places in the code below. In computer programming it is generally best to use variables and constants rather than having to hard-code specific numbers every time that number is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We processed image size to be 64\n",
    "img_size = 64\n",
    "\n",
    "# Number of channels: 1 because greyscale\n",
    "num_channels = 1\n",
    "\n",
    "# Number of digits\n",
    "num_digits = 5\n",
    "\n",
    "# Number of output labels\n",
    "num_labels = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network configuration\n",
    "\n",
    "The configuration of the Convolutional Neural Network is defined here for convenience, so you can easily find and change these numbers and re-run the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 32         # There are 36 of these filters.\n",
    "\n",
    "# Convolutional Layer 3\n",
    "filter_size3 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters3 = 64         # There are 48 of these filters.\n",
    "\n",
    "# Fully-connected layer\n",
    "fc_size = 64             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model\n",
    "\n",
    "Let's build our tensorflow model step-by-step. The entire purpose of TensorFlow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python.\n",
    "\n",
    "A TensorFlow graph consists of the following parts which will be detailed below:\n",
    "\n",
    "* Placeholder variables used to change the input to the graph.\n",
    "* Model variables that are going to be optimized\n",
    "* The model which is essentially just a mathematical function that calculates some output given the input in the placeholder variables and the model variables.\n",
    "* A cost measure that can be used to guide the optimization of the variables.\n",
    "* An optimization method which updates the variables of the model.\n",
    "\n",
    "### Placeholder Variables\n",
    "\n",
    "Placeholder variables serve as the input to the graph that we may change each time we execute the graph\n",
    "\n",
    "First we define the placeholder variable for the input images. This allows us to change the images that are input to the TensorFlow graph. This is a so-called tensor, which just means that it is a multi-dimensional vector or matrix. The data-type is set to float32 and the shape is set to [None, img_size_flat], where None means that the tensor may hold an arbitrary number of images with each image being a vector of length img_size_flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Images placeholder\n",
    "x = tf.placeholder(tf.float32, shape=(None, img_size, img_size, num_channels), name='x')\n",
    "\n",
    "# Labels placeholder\n",
    "y_true = tf.placeholder(tf.int64, shape=[None, num_digits], name='y_true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce overfitting, we will apply dropout before the readout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer 1\n",
    "\n",
    "Create the first convolutional layer. It takes x as input and creates num_filters1 different filters, each having width and height equal to filter_size1. Finally we wish to down-sample the image so it is half the size by using 2x2 max-pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_1, w_c1 = conv_layer(x, num_channels, filter_size1, num_filters1, 'w_c1', True)\n",
    "\n",
    "conv_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer 2\n",
    "\n",
    "Create the second convolutional layer, which takes as input the output from the first convolutional layer. The number of input channels corresponds to the number of filters in the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_2, w_c2 = conv_layer(conv_1, num_filters1, filter_size2, num_filters2, 'w_c2', True)\n",
    "\n",
    "conv_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_3, w_c3 = conv_layer(conv_2, num_filters2, filter_size3, num_filters3, 'w_c3', False)\n",
    "\n",
    "conv_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropout = tf.nn.dropout(conv_3, keep_prob)\n",
    "\n",
    "dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "\n",
    "The convolutional layers output 4-dim tensors. We now wish to use these as input in a fully-connected network, which requires for the tensors to be reshaped or flattened to 2-dim tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatten, num_features = flatten_layer(dropout)\n",
    "\n",
    "flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the tensors now have shape (?, 16384) which means there's an arbitrary number of images which have been flattened to vectors of length 16384 each. Note that 16384 = 16 x 16 x 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer 1\n",
    "\n",
    "Add a fully-connected layer to the network. The input is the flattened layer from the previous convolution. The number of neurons or nodes in the fully-connected layer is fc_size. ReLU is used so we can learn non-linear relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_1 = fc_layer(flatten, num_features, fc_size, 'w_fc1', relu=True)\n",
    "\n",
    "fc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the output of the fully-connected layer is a tensor with shape (?, 128) where the ? means there is an arbitrary number of images and fc_size == 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Class\n",
    "\n",
    "The fully-connected layer estimates how likely it is that the input image belongs to each of the 10 classes for each of the 5 digits. However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, so we want to normalize them so that each element is limited between zero and one. This is calculated using the so-called softmax function and the result is stored in y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits_1, w_s1 = softmax_function(fc_1, fc_size, num_labels, 'w_s1')\n",
    "logits_2, w_s2 = softmax_function(fc_1, fc_size, num_labels, 'w_s2')\n",
    "logits_3, w_s3 = softmax_function(fc_1, fc_size, num_labels, 'w_s3')\n",
    "logits_4, w_s4 = softmax_function(fc_1, fc_size, num_labels, 'w_s4')\n",
    "logits_5, w_s5 = softmax_function(fc_1, fc_size, num_labels, 'w_s5')\n",
    "\n",
    "y_pred = [logits_1, logits_2, logits_3, logits_4, logits_5]\n",
    "\n",
    "# The class-number is the index of the largest element.\n",
    "y_pred_cls = tf.transpose(tf.argmax(y_pred, dimension=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "To make the model better at classifying the input images, we must somehow change the variables for all the network layers. To do this we first need to know how well the model currently performs by comparing the predicted output of the model y_pred to the desired output y_true.\n",
    "\n",
    "The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. The goal of optimization is therefore to minimize the cross-entropy so it gets as close to zero as possible by changing the variables of the network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_1, y_true[:, 0]))\n",
    "loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_2, y_true[:, 1]))\n",
    "loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_3, y_true[:, 2]))\n",
    "loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_4, y_true[:, 3]))\n",
    "loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_5, y_true[:, 4]))\n",
    "\n",
    "loss = loss1 + loss2 + loss3 + loss4 + loss5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method\n",
    "\n",
    "Now that we have a cost measure that must be minimized, we can then create an optimizer. In this case it is the [``AdamOptimizer``](https://www.tensorflow.org/api_docs/python/train/optimizers#AdamOptimizer) which is an advanced form of Gradient Descent. When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an exponential decay function to a provided initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use global_step as a counter variable\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "# The learning rate is initially set to 0.05\n",
    "start_learning_rate = 0.1\n",
    "\n",
    "# Apply exponential decay to the learning rate\n",
    "learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 10000, 0.96)\n",
    "\n",
    "# Use the Adagrad optimizer\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "To evaluate the performance of our Convolutional Network we calculate the average accuracy across all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(predictions == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow Session\n",
    "\n",
    "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables for weights and biases must be initialized before we start optimizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "There are many images in the training-set. It takes a long time to calculate the gradient of the model using all these images. We therefore only use a small batch of images in each iteration of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Number of steps between each update\n",
    "display_step = 5000\n",
    "\n",
    "# Dropout\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to save and restore a model is to use a tf.train.Saver object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "save_dir = 'checkpoints/'\n",
    "\n",
    "# Create directory if it does not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "save_path = os.path.join(save_dir, 'svhn_multi')\n",
    "\n",
    "#'''If you want to restore model'''\n",
    "#saver.restore(sess=session, save_path=save_path)\n",
    "#print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, a new batch of data is selected from the training-set and then TensorFlow executes the optimizer using those training samples. The progress is printed every 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(num_iterations):\n",
    "\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict_train = {x: batch_data, y_true: batch_labels, keep_prob: dropout}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every x iterations.\n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            # Calculate the accuracy on the training-set.\n",
    "            batch_predictions = session.run(y_pred_cls, feed_dict=feed_dict_train)\n",
    "            print(\"Minibatch accuracy at step %d: %.4f\" % (step, accuracy(batch_predictions, batch_labels)))\n",
    "            \n",
    "            # Calculate the accuracy on the validation-set\n",
    "            val_predictions = session.run(y_pred_cls, {x: X_val, y_true: y_val, keep_prob: 1.})\n",
    "            print(\"Validation accuracy: %.4f\" % accuracy(val_predictions, y_val))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = time.time() - start_time\n",
    "    \n",
    "    # Calculate the accuracy on the test-set\n",
    "    test_predictions = session.run(y_pred_cls, {x: X_test, y_true: y_test, keep_prob: 1.})\n",
    "    \n",
    "    print(\"Test accuracy: %.4f\" % accuracy(test_predictions, y_test))\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "    \n",
    "    saver.save(sess=session, save_path=save_path)\n",
    "    print('Model saved in file: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run 100,000 iterations and see how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimize(num_iterations=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset performance\n",
    "\n",
    "Let's plot some of the mis-classified examples in our testset and a confusion matrix showing how well our model is able to predict the different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the testset\n",
    "test_pred = session.run(y_pred_cls, feed_dict={x: X_test, y_true: y_test, keep_prob: 1.0})\n",
    "\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly classified images\n",
    "\n",
    "Let's find some correctly classified examples and plot the true and predicted label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the incorrectly classified examples\n",
    "correct = np.array([(a==b).all() for a, b in zip(test_pred, y_test)])\n",
    "\n",
    "# Select the incorrectly classified examples\n",
    "images = X_test[correct]\n",
    "cls_true = y_test[correct]\n",
    "cls_pred = test_pred[correct]\n",
    "\n",
    "# Plot the mis-classified examples\n",
    "plot_images(images, 6, 6, cls_true, cls_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly classified images\n",
    "\n",
    "Let's invert the boolean array and plot some of the incorrectly classified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the incorrectly classified examples\n",
    "incorrect = np.invert(correct)\n",
    "\n",
    "# Select the incorrectly classified examples\n",
    "images = X_test[incorrect]\n",
    "cls_true = y_test[incorrect]\n",
    "cls_pred = test_pred[incorrect]\n",
    "\n",
    "# Plot the mis-classified examples\n",
    "plot_images(images, 6, 6, cls_true, cls_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
